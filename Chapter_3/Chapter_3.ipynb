{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 数据太大，无法上传\n",
    "## 地址是：http://evexdb.org/pmresources/vec-space-models/wikipedia-pubmed-and-PMC-w2v.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import csv\n",
    "import codecs \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow \n",
    "print(tensorflow.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "found 5443656 word vectors of words2vec\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_FILE = 'wikipedia-pubmed-and-PMC-w2v.bin'\n",
    "print('Indexing word vectors')\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE,binary=True)\n",
    "print('found %s word vectors of words2vec'% len(word2vec.vocab) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_FILE =  'DRUG-AE.rel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_ae = []\n",
    "op_labels_ae = []\n",
    "\n",
    "sentences = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(TEXT_FILE, 'r')\n",
    "\n",
    "# for each_line in f.readlines():\n",
    "#     sent_list = np.zeros([0,200])\n",
    "#     labels = np.zeros([0,3])\n",
    "#     tokens = each_line.split(\"|\")\n",
    "#     sent = tokens[1]\n",
    "#     if sent in sentences:\n",
    "#         continue\n",
    "#     sentences.append(sent)\n",
    "#     begin_offset = int(tokens[3])\n",
    "#     end_offset = int(tokens[4])\n",
    "#     mid_offset = range(begin_offset+1, end_offset)\n",
    "#     word_tokens = nltk.word_tokenize(sent)\n",
    "#     offset = 0\n",
    "#     for each_token in word_tokens:\n",
    "#         offset = sent.find(each_token, offset)\n",
    "#         offset1 = copy.deepcopy(offset)\n",
    "#         offset += len(each_token)\n",
    "#         if each_token in punctuation or re.search(r'\\d', each_token):\n",
    "#             continue\n",
    "#         each_token = each_token.lower()\n",
    "#         each_token = re.sub(\"[^A-Za-z\\-]+\",\"\", each_token)\n",
    "#         if each_token in word2vec.vocab:\n",
    "#             new_word = word2vec.word_vec(each_token)\n",
    "#         if offset1 == begin_offset:\n",
    "#             sent_list = np.append(sent_list, np.array([new_word]), axis=0)\n",
    "#             labels = np.append(labels, np.array([[0,0,1]]), axis=0)\n",
    "#         elif offset == end_offset or offset in mid_offset:\n",
    "#             sent_list = np.append(sent_list, np.array([new_word]), axis=0)\n",
    "#             labels = np.append(labels, np.array([[0,1,0]]), axis=0)\n",
    "#         else:\n",
    "#             sent_list = np.append(sent_list, np.array([new_word]), axis=0)\n",
    "#             labels = np.append(labels, np.array([[1,0,0]]), axis=0)\n",
    "\n",
    "#     input_data_ae.append(sent_list)\n",
    "#     op_labels_ae.append(labels)\n",
    "# input_data_ae = np.array(input_data_ae)\n",
    "# op_labels_ae  = np.array(op_labels_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(TEXT_FILE, 'r')\n",
    "\n",
    "for each_line in f.readlines():\n",
    "    sent_list = np.zeros([0,200])\n",
    "    labels = np.zeros([0,3])\n",
    "    tokens = each_line.split(\"|\")\n",
    "    sent = tokens[1]\n",
    "    if sent in sentences:\n",
    "        continue\n",
    "    sentences.append(sent)\n",
    "    begin_offset = int(tokens[3])\n",
    "    end_offset = int(tokens[4])\n",
    "    mid_offset = range(begin_offset+1, end_offset)\n",
    "    word_tokens = nltk.word_tokenize(sent)\n",
    "    offset = 0\n",
    "    for each_token in word_tokens:\n",
    "        offset = sent.find(each_token, offset)\n",
    "        offset1 = copy.deepcopy(offset)\n",
    "        offset += len(each_token)\n",
    "        if each_token in punctuation or re.search(r'\\d', each_token):\n",
    "            continue\n",
    "        each_token = each_token.lower()\n",
    "        each_token = re.sub(\"[^A-Za-z\\-]+\",\"\", each_token)\n",
    "        if each_token in word2vec.vocab:\n",
    "            new_word = word2vec.word_vec(each_token)\n",
    "        if offset1 == begin_offset:\n",
    "            sent_list = np.append(sent_list, np.array([new_word]), axis=0)\n",
    "            labels = np.append(labels, np.array([[0,0,1]]), axis=0)\n",
    "        elif offset == end_offset or offset in mid_offset:\n",
    "            sent_list = np.append(sent_list, np.array([new_word]), axis=0)\n",
    "            labels = np.append(labels, np.array([[0,1,0]]), axis=0)\n",
    "        else:\n",
    "            sent_list = np.append(sent_list, np.array([new_word]), axis=0)\n",
    "            labels = np.append(labels, np.array([[1,0,0]]), axis=0)\n",
    "\n",
    "    input_data_ae.append(sent_list)\n",
    "    op_labels_ae.append(labels)\n",
    "input_data_ae = np.array(input_data_ae)\n",
    "op_labels_ae  = np.array(op_labels_ae) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_ae = pad_sequences(input_data_ae,maxlen=30,dtype='float64',padding='post')\n",
    "op_labels_ae = pad_sequences(op_labels_ae,maxlen=30,dtype='float64',padding='post') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4271\n",
      "4271\n"
     ]
    }
   ],
   "source": [
    "print(len(input_data_ae))\n",
    "print(len(op_labels_ae)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense,Input,LSTM,Embedding,Dropout,Activation,Bidirectional,TimeDistributed\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = input_data_ae[:4000]\n",
    "x_test = input_data_ae[4000:]\n",
    "y_train = op_labels_ae[:4000]\n",
    "y_test = op_labels_ae[4000:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 1\n",
    "\n",
    "xin = Input(batch_shape=(batch,30,200),dtype='float')\n",
    "seq = Bidirectional(LSTM(300,return_sequences=True),merge_mode='concat')(xin)\n",
    "mlp1 = Dropout(0.2)(seq)\n",
    "mlp2 = TimeDistributed(Dense(60,activation='softmax'))(mlp1)\n",
    "mlp3 = Dropout(0.2)(mlp2)\n",
    "mlp4 = TimeDistributed(Dense(3,activation='softmax'))(mlp3) \n",
    "model = Model(inputs=xin,outputs=mlp4)\n",
    "model.compile(optimizer='Adam',loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 271 samples\n",
      "Epoch 1/50\n",
      "4000/4000 [==============================] - 198s 50ms/step - loss: 0.1664 - val_loss: 0.0948\n",
      "Epoch 2/50\n",
      "4000/4000 [==============================] - 214s 53ms/step - loss: 0.1000 - val_loss: 0.0840\n",
      "Epoch 3/50\n",
      "4000/4000 [==============================] - 191s 48ms/step - loss: 0.0890 - val_loss: 0.0747\n",
      "Epoch 4/50\n",
      "4000/4000 [==============================] - 203s 51ms/step - loss: 0.0788 - val_loss: 0.0696\n",
      "Epoch 5/50\n",
      "4000/4000 [==============================] - 191s 48ms/step - loss: 0.0691 - val_loss: 0.0645\n",
      "Epoch 6/50\n",
      "4000/4000 [==============================] - 194s 48ms/step - loss: 0.0614 - val_loss: 0.0620\n",
      "Epoch 7/50\n",
      "4000/4000 [==============================] - 194s 49ms/step - loss: 0.0535 - val_loss: 0.0634\n",
      "Epoch 8/50\n",
      "4000/4000 [==============================] - 194s 48ms/step - loss: 0.0465 - val_loss: 0.0686\n",
      "Epoch 9/50\n",
      "4000/4000 [==============================] - 188s 47ms/step - loss: 0.0418 - val_loss: 0.0706\n",
      "Epoch 10/50\n",
      "4000/4000 [==============================] - 188s 47ms/step - loss: 0.0365 - val_loss: 0.0676\n",
      "Epoch 11/50\n",
      "4000/4000 [==============================] - 196s 49ms/step - loss: 0.0322 - val_loss: 0.0754\n",
      "Epoch 12/50\n",
      "4000/4000 [==============================] - 199s 50ms/step - loss: 0.0290 - val_loss: 0.0819\n",
      "Epoch 13/50\n",
      "4000/4000 [==============================] - 186s 47ms/step - loss: 0.0255 - val_loss: 0.0891\n",
      "Epoch 14/50\n",
      "4000/4000 [==============================] - 184s 46ms/step - loss: 0.0224 - val_loss: 0.0958\n",
      "Epoch 15/50\n",
      "4000/4000 [==============================] - 175s 44ms/step - loss: 0.0211 - val_loss: 0.0901\n",
      "Epoch 16/50\n",
      "4000/4000 [==============================] - 196s 49ms/step - loss: 0.0186 - val_loss: 0.1062\n",
      "Epoch 17/50\n",
      "4000/4000 [==============================] - 204s 51ms/step - loss: 0.0161 - val_loss: 0.1216\n",
      "Epoch 18/50\n",
      "4000/4000 [==============================] - 203s 51ms/step - loss: 0.0155 - val_loss: 0.1134\n",
      "Epoch 19/50\n",
      "4000/4000 [==============================] - 203s 51ms/step - loss: 0.0147 - val_loss: 0.1156\n",
      "Epoch 20/50\n",
      "4000/4000 [==============================] - 203s 51ms/step - loss: 0.0131 - val_loss: 0.1197\n",
      "Epoch 21/50\n",
      "4000/4000 [==============================] - 202s 51ms/step - loss: 0.0129 - val_loss: 0.1273\n",
      "Epoch 22/50\n",
      "4000/4000 [==============================] - 199s 50ms/step - loss: 0.0117 - val_loss: 0.1181\n",
      "Epoch 23/50\n",
      "4000/4000 [==============================] - 167s 42ms/step - loss: 0.0114 - val_loss: 0.1117\n",
      "Epoch 24/50\n",
      "4000/4000 [==============================] - 168s 42ms/step - loss: 0.0096 - val_loss: 0.1287\n",
      "Epoch 25/50\n",
      "4000/4000 [==============================] - 168s 42ms/step - loss: 0.0089 - val_loss: 0.1212\n",
      "Epoch 26/50\n",
      "4000/4000 [==============================] - 166s 42ms/step - loss: 0.0094 - val_loss: 0.1355\n",
      "Epoch 27/50\n",
      "4000/4000 [==============================] - 167s 42ms/step - loss: 0.0073 - val_loss: 0.1370\n",
      "Epoch 28/50\n",
      "4000/4000 [==============================] - 167s 42ms/step - loss: 0.0070 - val_loss: 0.1472\n",
      "Epoch 29/50\n",
      "4000/4000 [==============================] - 168s 42ms/step - loss: 0.0067 - val_loss: 0.1460\n",
      "Epoch 30/50\n",
      "4000/4000 [==============================] - 167s 42ms/step - loss: 0.0067 - val_loss: 0.1445\n",
      "Epoch 31/50\n",
      "4000/4000 [==============================] - 176s 44ms/step - loss: 0.0071 - val_loss: 0.1314\n",
      "Epoch 32/50\n",
      "4000/4000 [==============================] - 199s 50ms/step - loss: 0.0067 - val_loss: 0.1346\n",
      "Epoch 33/50\n",
      "4000/4000 [==============================] - 195s 49ms/step - loss: 0.0063 - val_loss: 0.1345\n",
      "Epoch 34/50\n",
      "4000/4000 [==============================] - 198s 50ms/step - loss: 0.0055 - val_loss: 0.1324\n",
      "Epoch 35/50\n",
      "4000/4000 [==============================] - 185s 46ms/step - loss: 0.0055 - val_loss: 0.1355\n",
      "Epoch 36/50\n",
      "4000/4000 [==============================] - 181s 45ms/step - loss: 0.0056 - val_loss: 0.1323\n",
      "Epoch 37/50\n",
      "4000/4000 [==============================] - 167s 42ms/step - loss: 0.0059 - val_loss: 0.1409\n",
      "Epoch 38/50\n",
      "4000/4000 [==============================] - 183s 46ms/step - loss: 0.0051 - val_loss: 0.1481\n",
      "Epoch 39/50\n",
      "4000/4000 [==============================] - 188s 47ms/step - loss: 0.0051 - val_loss: 0.1487\n",
      "Epoch 40/50\n",
      "4000/4000 [==============================] - 187s 47ms/step - loss: 0.0049 - val_loss: 0.1377\n",
      "Epoch 41/50\n",
      "4000/4000 [==============================] - 192s 48ms/step - loss: 0.0045 - val_loss: 0.1490\n",
      "Epoch 42/50\n",
      "4000/4000 [==============================] - 190s 47ms/step - loss: 0.0044 - val_loss: 0.1527\n",
      "Epoch 43/50\n",
      "4000/4000 [==============================] - 189s 47ms/step - loss: 0.0044 - val_loss: 0.1493\n",
      "Epoch 44/50\n",
      "4000/4000 [==============================] - 208s 52ms/step - loss: 0.0049 - val_loss: 0.1430\n",
      "Epoch 45/50\n",
      "4000/4000 [==============================] - 194s 48ms/step - loss: 0.0040 - val_loss: 0.1596\n",
      "Epoch 46/50\n",
      "4000/4000 [==============================] - 186s 46ms/step - loss: 0.0034 - val_loss: 0.1709\n",
      "Epoch 47/50\n",
      "4000/4000 [==============================] - 193s 48ms/step - loss: 0.0046 - val_loss: 0.1589\n",
      "Epoch 48/50\n",
      "4000/4000 [==============================] - 213s 53ms/step - loss: 0.0040 - val_loss: 0.1560\n",
      "Epoch 49/50\n",
      "4000/4000 [==============================] - 219s 55ms/step - loss: 0.0038 - val_loss: 0.1640\n",
      "Epoch 50/50\n",
      "4000/4000 [==============================] - 222s 56ms/step - loss: 0.0043 - val_loss: 0.1547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a393180f0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,\n",
    "         batch_size=batch,\n",
    "         epochs=50,\n",
    "         validation_data=(x_test,y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model.predict(x_test,batch_size=batch)\n",
    "labels = []\n",
    "for i in range(len(val_pred)):\n",
    "    b = np.zeros_like(val_pred[i])\n",
    "    b[np.arange(len(val_pred[i])),val_pred[i].argmax(1)] = 1\n",
    "    labels.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(271, 30, 3)\n"
     ]
    }
   ],
   "source": [
    "print(val_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "f1 = []\n",
    "precision = []\n",
    "recall = []\n",
    "point = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lidianxiang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/lidianxiang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/lidianxiang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/lidianxiang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y_test)):\n",
    "    if (f1_score(labels[i],y_test[i],average='weighted') >.6):\n",
    "        point.append(i)\n",
    "    score.append(f1_score(labels[i],y_test[i],average='weighted'))\n",
    "    precision.append(precision_score(labels[i],y_test[i],average='weighted'))\n",
    "    recall.append(recall_score(labels[i],y_test[i],average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.26568265682657\n"
     ]
    }
   ],
   "source": [
    "print(len(point)/len(labels)*100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6873310922921418\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9741375645016355\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(precision)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5769987699876998\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(recall)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7125000000000001, 0.8235294117647058, 0.32727272727272727, 0.888888888888889, 0.4583333333333333, 0.6046511627906976, 0.888888888888889, 0.7144927536231884, 0.17413793103448275, 0.983050847457627, 0.8, 1.0, 0.7234042553191489, 0.4272727272727272, 0.8679245283018869, 0.7234042553191489, 0.5864864864864866, 0.9502824858757061, 0.8113378684807256, 0.6046511627906976, 0.8, 0.8976190476190476, 0.21269841269841272, 1.0, 0.4272727272727272, 0.9655172413793104, 0.7499999999999999, 0.3929824561403508, 0.7755102040816326, 0.418018018018018, 0.7891156462585034, 1.0, 0.6666666666666666, 0.4814814814814815, 0.17901234567901234, 0.7499999999999999, 0.4210526315789474, 0.4615384615384615, 0.6956521739130436, 0.5907859078590786, 0.888888888888889, 0.8679245283018869, 0.5864864864864866, 0.5714285714285715, 0.6666666666666666, 0.3, 0.5, 0.5714285714285715, 0.6666666666666666, 0.7234042553191489, 0.7755102040816326, 0.5904761904761905, 0.15777777777777777, 0.7499999999999999, 0.9285714285714286, 0.35272727272727267, 0.9473684210526316, 0.7234042553191489, 0.4814814814814815, 0.5365853658536585, 0.8, 1.0, 0.45135135135135135, 0.9285714285714286, 0.418018018018018, 0.6363636363636364, 0.7960784313725491, 0.8, 0.5777777777777777, 0.52, 0.10651340996168582, 1.0, 0.41290322580645167, 0.9285714285714286, 0.6222222222222221, 0.5008130081300812, 0.8235294117647058, 0.8, 0.4307692307692308, 0.4615384615384615, 0.5, 0.6046511627906976, 0.6046511627906976, 0.5536842105263157, 0.37592592592592594, 0.5, 0.7234042553191489, 0.6956521739130436, 0.7755102040816326, 0.5, 0.21458333333333332, 0.6956521739130436, 0.8679245283018869, 1.0, 1.0, 1.0, 0.7146341463414634, 0.8179487179487179, 0.700348432055749, 0.9473684210526316, 1.0, 0.888888888888889, 0.489247311827957, 1.0, 0.9285714285714286, 0.705185185185185, 0.7755102040816326, 0.6956521739130436, 0.3179723502304147, 1.0, 0.8, 0.4615384615384615, 0.9502824858757061, 0.9285714285714286, 0.888888888888889, 0.42631578947368415, 0.9655172413793104, 0.888888888888889, 0.8, 0.17714285714285716, 0.5, 0.5314285714285714, 0.8679245283018869, 0.6206349206349207, 0.7234042553191489, 0.7499999999999999, 0.2761904761904762, 0.45135135135135135, 0.7234042553191489, 0.983050847457627, 0.20347222222222222, 0.8030476190476191, 0.2571428571428571, 0.6666666666666666, 0.846153846153846, 0.5357142857142857, 0.8679245283018869, 0.9321428571428572, 0.9473684210526316, 0.6956521739130436, 0.6956521739130436, 0.23529411764705882, 0.9655172413793104, 0.8418300653594771, 0.29032258064516125, 0.888888888888889, 0.6874074074074075, 0.6209790209790209, 0.9655172413793104, 0.5134502923976608, 0.7499999999999999, 0.33616161616161616, 0.846153846153846, 0.7234042553191489, 0.38125, 0.7499999999999999, 0.307258064516129, 0.8235294117647058, 0.8679245283018869, 0.2857142857142857, 1.0, 1.0, 0.9473684210526316, 0.8, 0.7499999999999999, 0.6666666666666666, 0.6046511627906976, 0.6924242424242425, 0.8679245283018869, 0.37837837837837834, 0.7234042553191489, 0.5649999999999998, 0.846153846153846, 0.9090909090909091, 0.983050847457627, 0.49849849849849853, 0.6956521739130436, 0.6956521739130436, 0.888888888888889, 0.9473684210526316, 0.2555555555555556, 0.846153846153846, 0.846153846153846, 0.888888888888889, 0.39607843137254906, 0.7234042553191489, 0.7755102040816326, 0.5, 0.6046511627906976, 0.9655172413793104, 0.6956521739130436, 0.6010526315789473, 0.8235294117647058, 0.6363636363636364, 0.9090909090909091, 0.4272727272727272, 0.6363636363636364, 0.6666666666666666, 0.7755102040816326, 1.0, 0.34949494949494947, 0.4615384615384615, 0.7755102040816326, 0.27173913043478254, 0.7499999999999999, 1.0, 0.42936507936507934, 1.0, 0.888888888888889, 0.9473684210526316, 0.40404040404040403, 0.6666666666666666, 0.27450980392156865, 0.9473684210526316, 0.8235294117647058, 0.4272727272727272, 0.6222222222222221, 0.8679245283018869, 1.0, 0.983050847457627, 0.8, 0.846153846153846, 0.888888888888889, 0.4615384615384615, 0.4210526315789474, 0.7499999999999999, 0.7755102040816326, 0.6666666666666666, 0.6363636363636364, 0.5365853658536585, 0.6956521739130436, 0.888888888888889, 0.42631578947368415, 0.768627450980392, 0.6666666666666666, 0.846153846153846, 0.7755102040816326, 0.22745098039215683, 0.6452032520325204, 0.8235294117647058, 1.0, 0.25490196078431376, 1.0, 0.4266666666666667, 0.7755102040816326, 0.9285714285714286, 0.8787878787878788, 0.25499999999999995, 0.9361616161616162, 0.3647619047619048, 0.9502824858757061, 0.7234042553191489, 1.0, 0.6956521739130436, 1.0, 0.3976470588235294, 0.7234042553191489, 0.888888888888889, 0.888888888888889, 0.9655172413793104, 0.5523809523809524, 0.7755102040816326, 1.0, 0.9090909090909091, 0.4615384615384615, 0.6046511627906976, 0.28559139784946236, 0.6363636363636364, 0.7755102040816326, 0.6956521739130436, 0.4592592592592593]\n",
      "\n",
      "---------------x--------------\n",
      "\n",
      "[0.8142857142857143, 1.0, 0.9666666666666667, 1.0, 1.0, 1.0, 1.0, 0.9666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9344444444444445, 0.9833333333333333, 1.0, 1.0, 0.9308641975308641, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7466666666666667, 1.0, 0.9666666666666667, 0.9666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9282051282051282, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8133333333333334, 0.9666666666666667, 1.0, 1.0, 0.85, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9666666666666667, 1.0, 0.9227272727272727, 1.0, 0.6842105263157895, 0.7428571428571428, 0.5166666666666667, 1.0, 1.0, 1.0, 0.9333333333333333, 0.7897435897435898, 1.0, 1.0, 0.7636363636363637, 1.0, 1.0, 1.0, 1.0, 0.9181818181818181, 0.9666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9246376811594204, 0.975, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 1.0, 1.0, 0.975, 1.0, 1.0, 1.0, 0.9344444444444445, 1.0, 1.0, 0.7363636363636364, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9416666666666667, 1.0, 0.9833333333333333, 1.0, 1.0, 0.8055555555555556, 1.0, 1.0, 1.0, 0.9833333333333333, 0.9042424242424242, 0.5625, 1.0, 1.0, 0.975, 1.0, 0.9666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 0.76, 1.0, 0.9666666666666667, 0.9714285714285714, 1.0, 0.95, 1.0, 0.9777777777777777, 1.0, 1.0, 1.0, 1.0, 0.9600000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9666666666666667, 1.0, 1.0, 1.0, 1.0, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9533333333333334, 1.0, 1.0, 1.0, 0.8761904761904762, 1.0, 0.7777777777777779, 1.0, 1.0, 1.0, 0.9333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7363636363636364, 0.8521739130434782, 1.0, 1.0, 1.0, 0.7733333333333334, 0.9777777777777777, 1.0, 1.0, 0.5416666666666666, 1.0, 0.9777777777777777, 1.0, 1.0, 0.9294871794871795, 0.9600000000000001, 0.9833333333333333, 1.0, 0.9344444444444445, 1.0, 1.0, 1.0, 1.0, 0.8714285714285714, 1.0, 1.0, 1.0, 1.0, 0.8923076923076924, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9444444444444444, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "---------------x--------------\n",
      "\n",
      "[0.6333333333333333, 0.7, 0.23333333333333334, 0.8, 0.3333333333333333, 0.43333333333333335, 0.8, 0.5666666666666667, 0.1, 0.9666666666666667, 0.6666666666666666, 1.0, 0.5666666666666667, 0.3, 0.7666666666666667, 0.5666666666666667, 0.43333333333333335, 0.9666666666666667, 0.7, 0.43333333333333335, 0.6666666666666666, 0.8666666666666667, 0.13333333333333333, 1.0, 0.3, 0.9333333333333333, 0.6, 0.26666666666666666, 0.6333333333333333, 0.26666666666666666, 0.6666666666666666, 1.0, 0.5, 0.3333333333333333, 0.1, 0.6, 0.26666666666666666, 0.3, 0.5333333333333333, 0.43333333333333335, 0.8, 0.7666666666666667, 0.43333333333333335, 0.4, 0.5, 0.2, 0.3333333333333333, 0.4, 0.5, 0.5666666666666667, 0.6333333333333333, 0.4666666666666667, 0.1, 0.6, 0.8666666666666667, 0.23333333333333334, 0.9, 0.5666666666666667, 0.3333333333333333, 0.36666666666666664, 0.6666666666666666, 1.0, 0.3, 0.8666666666666667, 0.26666666666666666, 0.4666666666666667, 0.7, 0.6666666666666666, 0.49999999999999994, 0.4, 0.06666666666666667, 1.0, 0.3, 0.8666666666666667, 0.4666666666666667, 0.36666666666666664, 0.7, 0.6666666666666666, 0.3, 0.3, 0.3333333333333333, 0.43333333333333335, 0.43333333333333335, 0.4, 0.23333333333333334, 0.3333333333333333, 0.5666666666666667, 0.5333333333333333, 0.6333333333333333, 0.3333333333333333, 0.13333333333333333, 0.5333333333333333, 0.7666666666666667, 1.0, 1.0, 1.0, 0.5666666666666667, 0.7333333333333333, 0.5666666666666667, 0.9, 1.0, 0.8, 0.36666666666666664, 1.0, 0.8666666666666667, 0.5666666666666667, 0.6333333333333333, 0.5333333333333333, 0.23333333333333334, 1.0, 0.6666666666666666, 0.3, 0.9666666666666667, 0.8666666666666667, 0.8, 0.3, 0.9333333333333333, 0.8, 0.6666666666666666, 0.1, 0.3333333333333333, 0.4, 0.7666666666666667, 0.4666666666666667, 0.5666666666666667, 0.6, 0.16666666666666666, 0.3, 0.5666666666666667, 0.9666666666666667, 0.13333333333333333, 0.7333333333333333, 0.16666666666666666, 0.5, 0.7333333333333333, 0.4, 0.7666666666666667, 0.9, 0.9, 0.5333333333333333, 0.5333333333333333, 0.13333333333333333, 0.9333333333333333, 0.7666666666666667, 0.2, 0.8, 0.5333333333333333, 0.5, 0.9333333333333333, 0.36666666666666664, 0.6, 0.23333333333333334, 0.7333333333333333, 0.5666666666666667, 0.26666666666666666, 0.6, 0.23333333333333334, 0.7, 0.7666666666666667, 0.16666666666666666, 1.0, 1.0, 0.9, 0.6666666666666666, 0.6, 0.5, 0.43333333333333335, 0.5333333333333333, 0.7666666666666667, 0.23333333333333334, 0.5666666666666667, 0.4, 0.7333333333333333, 0.8333333333333334, 0.9666666666666667, 0.36666666666666664, 0.5333333333333333, 0.5333333333333333, 0.8, 0.9, 0.16666666666666666, 0.7333333333333333, 0.7333333333333333, 0.8, 0.26666666666666666, 0.5666666666666667, 0.6333333333333333, 0.3333333333333333, 0.43333333333333335, 0.9333333333333333, 0.5333333333333333, 0.43333333333333335, 0.7, 0.4666666666666667, 0.8333333333333334, 0.3, 0.4666666666666667, 0.5, 0.6333333333333333, 1.0, 0.23333333333333334, 0.3, 0.6333333333333333, 0.16666666666666666, 0.6, 1.0, 0.3, 1.0, 0.8, 0.9, 0.26666666666666666, 0.5, 0.16666666666666666, 0.9, 0.7, 0.3, 0.4666666666666667, 0.7666666666666667, 1.0, 0.9666666666666667, 0.6666666666666666, 0.7333333333333333, 0.8, 0.3, 0.26666666666666666, 0.6, 0.6333333333333333, 0.5, 0.4666666666666667, 0.36666666666666664, 0.5333333333333333, 0.8, 0.3, 0.7, 0.5, 0.7333333333333333, 0.6333333333333333, 0.13333333333333333, 0.5, 0.7, 1.0, 0.16666666666666666, 1.0, 0.3, 0.6333333333333333, 0.8666666666666667, 0.8333333333333334, 0.2, 0.9, 0.23333333333333334, 0.9666666666666667, 0.5666666666666667, 1.0, 0.5333333333333333, 1.0, 0.26666666666666666, 0.5666666666666667, 0.8, 0.8, 0.9333333333333333, 0.4, 0.6333333333333333, 1.0, 0.8333333333333334, 0.3, 0.43333333333333335, 0.2, 0.4666666666666667, 0.6333333333333333, 0.5333333333333333, 0.3]\n"
     ]
    }
   ],
   "source": [
    "print(score)\n",
    "print(\"\\n---------------x--------------\\n\")\n",
    "print(precision)\n",
    "print(\"\\n---------------x--------------\\n\")\n",
    "print(recall)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
